{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion Pipeline\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load markdown files from the data directory\n",
    "2. Chunk the text into manageable pieces\n",
    "3. Ingest the chunks into Qdrant via the API (which handles embedding generation)\n",
    "\n",
    "**IMPORTANT**: Make sure the API server is running before using this notebook!\n",
    "- Run: `make dev` or `docker compose up`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API URL: http://localhost:8000\n",
      "Collection: temp\n",
      "Data directory: ../../data\n"
     ]
    }
   ],
   "source": [
    "# API Configuration\n",
    "API_BASE_URL = \"http://localhost:8000\"\n",
    "COLLECTION_NAME = \"temp\"\n",
    "\n",
    "# Data paths\n",
    "DATA_DIR = \"../../data\"\n",
    "\n",
    "# Chunking settings\n",
    "CHUNK_SIZE = 500  # characters\n",
    "CHUNK_OVERLAP = 50  # characters\n",
    "\n",
    "print(f\"API URL: {API_BASE_URL}\")\n",
    "print(f\"Collection: {COLLECTION_NAME}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ API is running\n",
      "  Version: 0.1.0\n",
      "  Timestamp: 1771347119.3295116\n"
     ]
    }
   ],
   "source": [
    "# Create HTTP client\n",
    "client = httpx.Client(timeout=60.0)\n",
    "\n",
    "# Check API health\n",
    "try:\n",
    "    response = client.get(f\"{API_BASE_URL}/v1/health\")\n",
    "    response.raise_for_status()\n",
    "    health = response.json()\n",
    "    print(f\"✓ API is running\")\n",
    "    print(f\"  Version: {health['version']}\")\n",
    "    print(f\"  Timestamp: {health['timestamp']}\")\n",
    "except httpx.ConnectError:\n",
    "    print(\"✗ Cannot connect to API!\")\n",
    "    print(\"Make sure the API is running: make dev\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_markdown_files(data_dir: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Load all markdown files from the data directory.\"\"\"\n",
    "    documents = []\n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    for md_file in data_path.glob(\"*.md\"):\n",
    "        with open(md_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "            documents.append({\n",
    "                \"filename\": md_file.name,\n",
    "                \"filepath\": str(md_file.absolute()),\n",
    "                \"content\": content\n",
    "            })\n",
    "    \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, chunk_size: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"\n",
    "    Split text into overlapping chunks based on character count.\n",
    "    Uses paragraph-aware chunking for better semantic boundaries.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to chunk\n",
    "        chunk_size: Maximum characters per chunk\n",
    "        overlap: Number of overlapping characters between chunks\n",
    "    \n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    \"\"\"\n",
    "    # Split by double newline (paragraphs) first\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para = para.strip()\n",
    "        if not para:\n",
    "            continue\n",
    "        \n",
    "        # If adding this paragraph exceeds chunk size and we have content\n",
    "        if len(current_chunk) + len(para) > chunk_size and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            # Start new chunk with overlap\n",
    "            overlap_start = max(0, len(current_chunk) - overlap)\n",
    "            current_chunk = current_chunk[overlap_start:] + \"\\n\\n\" + para\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                current_chunk += \"\\n\\n\" + para\n",
    "            else:\n",
    "                current_chunk = para\n",
    "    \n",
    "    # Add remaining chunk\n",
    "    if current_chunk.strip():\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks if chunks else [text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created collection 'temp'\n"
     ]
    }
   ],
   "source": [
    "# Create collection (or use existing)\n",
    "try:\n",
    "    response = client.post(\n",
    "        f\"{API_BASE_URL}/v1/collections/\",\n",
    "        json={\n",
    "            \"name\": COLLECTION_NAME,\n",
    "            \"dimension\": 384,  # all-MiniLM-L6-v2 dimension\n",
    "            \"distance_metric\": \"cosine\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 201:\n",
    "        print(f\"✓ Created collection '{COLLECTION_NAME}'\")\n",
    "    elif response.status_code == 400 and \"already exists\" in response.text.lower():\n",
    "        print(f\"✓ Collection '{COLLECTION_NAME}' already exists\")\n",
    "    else:\n",
    "        print(f\"Response: {response.status_code} - {response.text}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2 markdown files:\n",
      "  - sample3.md (1896 characters)\n",
      "  - sample1.md (3297 characters)\n"
     ]
    }
   ],
   "source": [
    "# Load markdown files\n",
    "documents = load_markdown_files(DATA_DIR)\n",
    "print(f\"Loaded {len(documents)} markdown files:\")\n",
    "for doc in documents:\n",
    "    print(f\"  - {doc['filename']} ({len(doc['content'])} characters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First document preview:\n",
      "================================================================================\n",
      "# FastAPI: Modern Python Web Framework\n",
      "\n",
      "FastAPI is a modern, high-performance web framework for building APIs with Python. It is based on standard Python type hints and provides automatic data validation, serialization, and interactive API documentation.\n",
      "\n",
      "## Key Features\n",
      "\n",
      "- **High Performance**: FastAPI is one of the fastest Python frameworks, comparable to NodeJS and Go, thanks to its use of Starlette for the web parts and Pydantic for data handling.\n",
      "- **Type Safety**: Leverages Python type hin\n",
      "...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Preview first document\n",
    "if documents:\n",
    "    print(\"\\nFirst document preview:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(documents[0]['content'][:500])\n",
    "    print(\"...\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunk Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 15\n",
      "\n",
      "Chunks per document:\n",
      "  - sample3.md: 5 chunks\n",
      "  - sample1.md: 10 chunks\n"
     ]
    }
   ],
   "source": [
    "# Chunk all documents\n",
    "all_chunks = []\n",
    "\n",
    "for doc in documents:\n",
    "    chunks = chunk_text(doc['content'], CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "    \n",
    "    for idx, chunk in enumerate(chunks):\n",
    "        all_chunks.append({\n",
    "            \"text\": chunk,\n",
    "            \"filename\": doc['filename'],\n",
    "            \"filepath\": doc['filepath'],\n",
    "            \"chunk_index\": idx,\n",
    "            \"total_chunks\": len(chunks)\n",
    "        })\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"\\nChunks per document:\")\n",
    "for doc in documents:\n",
    "    doc_chunks = [c for c in all_chunks if c['filename'] == doc['filename']]\n",
    "    print(f\"  - {doc['filename']}: {len(doc_chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample chunk:\n",
      "================================================================================\n",
      "File: sample3.md\n",
      "Chunk: 1/5\n",
      "Length: 271 chars\n",
      "\n",
      "Text:\n",
      "# FastAPI: Modern Python Web Framework\n",
      "\n",
      "FastAPI is a modern, high-performance web framework for building APIs with Python. It is based on standard Python type hints and provides automatic data validation, serialization, and interactive API documentation.\n",
      "\n",
      "## Key Features...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Preview a chunk\n",
    "if all_chunks:\n",
    "    print(\"\\nSample chunk:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"File: {all_chunks[0]['filename']}\")\n",
    "    print(f\"Chunk: {all_chunks[0]['chunk_index'] + 1}/{all_chunks[0]['total_chunks']}\")\n",
    "    print(f\"Length: {len(all_chunks[0]['text'])} chars\")\n",
    "    print(f\"\\nText:\\n{all_chunks[0]['text'][:300]}...\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datapoints for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 15 datapoints for ingestion\n",
      "\n",
      "Sample datapoint:\n",
      "{'text': '# FastAPI: Modern Python Web Framework\\n\\nFastAPI is a modern, high-performance web framework for building APIs with Python. It is based on standard Python type hints and provides automatic data validation, serialization, and interactive API documentation.\\n\\n## Key Features', 'metadata': {'filename': 'sample3.md', 'filepath': '/Users/steffen/rag_template/backend/notebooks/../../data/sample3.md', 'chunk_index': 0, 'total_chunks': 5}}\n"
     ]
    }
   ],
   "source": [
    "# Prepare datapoints for bulk insert\n",
    "# The API will automatically generate embeddings for each text\n",
    "datapoints = []\n",
    "\n",
    "for chunk in all_chunks:\n",
    "    datapoints.append({\n",
    "        \"text\": chunk['text'],\n",
    "        \"metadata\": {\n",
    "            \"filename\": chunk['filename'],\n",
    "            \"filepath\": chunk['filepath'],\n",
    "            \"chunk_index\": chunk['chunk_index'],\n",
    "            \"total_chunks\": chunk['total_chunks']\n",
    "        }\n",
    "    })\n",
    "\n",
    "print(f\"Prepared {len(datapoints)} datapoints for ingestion\")\n",
    "print(f\"\\nSample datapoint:\")\n",
    "print(datapoints[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest into Qdrant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingesting 15 datapoints in batches of 50...\n",
      "\n",
      "  ✓ Batch 1/1: Inserted 15 datapoints\n",
      "\n",
      "✓ Ingestion complete! Total inserted: 15 datapoints\n"
     ]
    }
   ],
   "source": [
    "# Ingest in batches\n",
    "BATCH_SIZE = 50\n",
    "total_inserted = 0\n",
    "\n",
    "print(f\"Ingesting {len(datapoints)} datapoints in batches of {BATCH_SIZE}...\\n\")\n",
    "\n",
    "for i in range(0, len(datapoints), BATCH_SIZE):\n",
    "    batch = datapoints[i:i + BATCH_SIZE]\n",
    "    batch_num = i // BATCH_SIZE + 1\n",
    "    total_batches = (len(datapoints) + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    \n",
    "    try:\n",
    "        response = client.post(\n",
    "            f\"{API_BASE_URL}/v1/collections/{COLLECTION_NAME}/datapoints/bulk\",\n",
    "            json=batch\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        inserted = result.get(\"inserted_count\", len(batch))\n",
    "        total_inserted += inserted\n",
    "        print(f\"  ✓ Batch {batch_num}/{total_batches}: Inserted {inserted} datapoints\")\n",
    "        \n",
    "    except httpx.HTTPStatusError as e:\n",
    "        print(f\"  ✗ Batch {batch_num}/{total_batches} failed: {e}\")\n",
    "        print(f\"    Response: {e.response.text}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n✓ Ingestion complete! Total inserted: {total_inserted} datapoints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test query: What is RAG and how does it work?\n",
      "\n",
      "Top 3 results:\n",
      "\n",
      "1. Score: 7.6712\n",
      "   File: sample1.md\n",
      "   Chunk: 1/10\n",
      "   Text: # Retrieval-Augmented Generation (RAG)\n",
      "\n",
      "Retrieval-Augmented Generation (RAG) is an AI framework that enhances large language model (LLM) outputs by incorporating external knowledge retrieval into the ...\n",
      "\n",
      "2. Score: 3.1827\n",
      "   File: sample1.md\n",
      "   Chunk: 4/10\n",
      "   Text: in the retrieved information.\n",
      "\n",
      "## Benefits of RAG\n",
      "\n",
      "- **Reduced hallucinations**: By grounding responses in actual documents, RAG significantly reduces the tendency of LLMs to generate factually incorr...\n",
      "\n",
      "3. Score: 1.3304\n",
      "   File: sample1.md\n",
      "   Chunk: 2/10\n",
      "   Text: s\n",
      "\n",
      "The RAG pipeline consists of three main stages:\n",
      "\n",
      "1. **Indexing**: Documents are preprocessed, split into chunks, and converted into vector embeddings. These embeddings are stored in a vector databa...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test query using the full RAG pipeline\n",
    "test_query = \"What is RAG and how does it work?\"\n",
    "print(f\"Test query: {test_query}\\n\")\n",
    "\n",
    "try:\n",
    "    response = client.post(\n",
    "        f\"{API_BASE_URL}/v1/query/\",\n",
    "        json={\n",
    "            \"question\": test_query,\n",
    "            \"collection_name\": COLLECTION_NAME,\n",
    "            \"n_retrieval\": 5,\n",
    "            \"n_ranking\": 3\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    results = response.json()\n",
    "    \n",
    "    print(f\"Top {len(results['results'])} results:\\n\")\n",
    "    for i, result in enumerate(results['results'], 1):\n",
    "        print(f\"{i}. Score: {result['score']:.4f}\")\n",
    "        print(f\"   File: {result['metadata'].get('filename', 'N/A')}\")\n",
    "        print(f\"   Chunk: {result['metadata'].get('chunk_index', 0) + 1}/{result['metadata'].get('total_chunks', 1)}\")\n",
    "        print(f\"   Text: {result['text'][:200]}...\")\n",
    "        print()\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error during query: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Full RAG (with Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What are the benefits of RAG?\n",
      "\n",
      "Answer:\n",
      "================================================================================\n",
      "[chain_of_thought]\n",
      "The question is \"What are the benefits of RAG?\".\n",
      "The context provides a section titled \"Benefits of RAG\" that lists the benefits of RAG.\n",
      "I will combine the question with the information in the context to create an enhanced question.\n",
      "\n",
      "[Enhanced_Question]\n",
      "According to the provided information, what are the benefits of Retrieval-Augmented Generation (RAG)?\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test the full RAG pipeline (retrieval + generation)\n",
    "test_question = \"What are the benefits of RAG?\"\n",
    "print(f\"Question: {test_question}\\n\")\n",
    "\n",
    "try:\n",
    "    response = client.post(\n",
    "        f\"{API_BASE_URL}/v1/rag/\",\n",
    "        json={\n",
    "            \"question\": test_question,\n",
    "            \"collection_name\": COLLECTION_NAME,\n",
    "            \"n_retrieval\": 5,\n",
    "            \"n_ranking\": 3\n",
    "        }\n",
    "    )\n",
    "    response.raise_for_status()\n",
    "    result = response.json()\n",
    "    \n",
    "    print(\"Answer:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(result['answer'])\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during RAG: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ HTTP client closed\n"
     ]
    }
   ],
   "source": [
    "# Close HTTP client\n",
    "client.close()\n",
    "print(\"✓ HTTP client closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
